version: "2.1"
volumes:
  grafana:
  postgres:
  vmdata:
services:
  # Grafana agent batches and processes traces sent to it, generating
  # auto-logs from those traces.
  agent:
    image: grafana/agent:v0.37.4
    ports:
      - "12347:12345"
      - "12348:12348"
      - "6832:6832"
      - "55679:55679"
    volumes:
      # To run using a Static configuration instead, uncomment out the following line.
      - "./agent/config.yaml:/etc/agent/agent.yaml"
      # To run using a Static configuration instead, comment the following line.
      # - "./agent/config.river:/etc/agent/config.river"
    # To run using a Static configuration instead, uncomment out the following block.
    command: [
      "-config.file=/etc/agent/agent.yaml",
      "-server.http.address=0.0.0.0:12345",
    ]
    # To run using a Static configuration instead, comment the following `environment` and
    # `command` blocks.
    # environment:
    #  - AGENT_MODE=flow
    # command: [
    #  "run",
    #  "--server.http.listen-addr=0.0.0.0:12345",
    #  "/etc/agent/config.river",
    #]

  # The Grafana dashboarding server.
  grafana:
    image: grafana/grafana:10.2.1
    volumes:
      - "./grafana/definitions:/var/lib/grafana/dashboards"
      - "./grafana/provisioning:/etc/grafana/provisioning"
    ports:
      - "3000:3000"
    environment:
      - GF_FEATURE_TOGGLES_ENABLE=flameGraph traceqlSearch traceQLStreaming correlations metricsSummary traceqlEditor
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true

  # This creates a writeable Tempo data source and a couple of correlations.
  # NOTE: This relies on the Tempo data source not existing at first start, should you delete
  #       the Tempo data source and re-run this service (via a `docker compose up`), extra
  #       copies of correlations will be generated!
  grafana_api_data:
    build: ./curl
    depends_on:
      - grafana

  # A RabbitMQ queue used to send message between the requester and the server microservices.
  mythical-queue:
    image: rabbitmq:management
    restart: always
    ports:
      - "5672:5672"
      - "15672:15672"
    healthcheck:
      test: rabbitmq-diagnostics check_running
      interval: 5s
      timeout: 30s
      retries: 10

  # A postgres DB used to store data by the API server microservice.
  mythical-database:
    image: postgres:14.5
    restart: always
    environment:
      POSTGRES_PASSWORD: "mythical"
    volumes:
      - "postgres:/var/lib/postgresql/data"
    ports:
      - "5432:5432"

  # A microservice that makes requests to the API server microservice. Requests are also pushed onto the mythical-queue.
  mythical-requester:
    #build:
    #  context: ./source
    #  dockerfile: docker/Dockerfile
    #  args:
    #    SERVICE: mythical-beasts-requester
    image: grafana/intro-to-mltp:mythical-beasts-requester-latest
    depends_on:
      mythical-queue:
        condition: service_healthy
      mythical-server:
        condition: service_started
    ports:
      - "4001:4001"
    environment:
      - NAMESPACE=production
      - LOGS_TARGET=http://loki:3100/loki/api/v1/push
      - TRACING_COLLECTOR_HOST=agent
      - TRACING_COLLECTOR_PORT=4317
      - OTEL_EXPORTER_OTLP_TRACES_INSECURE=true
      - OTEL_RESOURCE_ATTRIBUTES=ip=1.2.3.4
      - ENDPOINT_TYPE=BORING

  # The API server microservice.
  # It writes logs directly to the Loki service, exposes metrics for the Prometheus
  # service and sends traces to the Grafana Agent instance.
  mythical-server:
    #build:
    #  context: ./source
    #  dockerfile: docker/Dockerfile
    #  args:
    #    SERVICE: mythical-beasts-server
    image: grafana/intro-to-mltp:mythical-beasts-server-latest
    ports:
      - "4000:4000"
      - "80:80"
    depends_on:
      - mythical-database
    environment:
      - NAMESPACE=production
      - LOGS_TARGET=http://loki:3100/loki/api/v1/push
      - TRACING_COLLECTOR_HOST=agent
      - TRACING_COLLECTOR_PORT=4317
      - OTEL_EXPORTER_OTLP_TRACES_INSECURE=true
      - OTEL_RESOURCE_ATTRIBUTES=ip=1.2.3.5
      - ENDPOINT_TYPE=BORING

  # A microservice that consumes requests from the mythical-queue
  mythical-recorder:
    #build:
    #  context: ./source
    #  dockerfile: docker/Dockerfile
    #  args:
    #    SERVICE: mythical-beasts-recorder
    image: grafana/intro-to-mltp:mythical-beasts-recorder-latest
    depends_on:
      mythical-queue:
        condition: service_healthy
    ports:
      - "4002:4002"
    environment:
      - NAMESPACE=production
      - LOGS_TARGET=http://loki:3100/loki/api/v1/push
      - TRACING_COLLECTOR_HOST=agent
      - TRACING_COLLECTOR_PORT=4317
      - OTEL_EXPORTER_OTLP_TRACES_INSECURE=true
      - OTEL_RESOURCE_ATTRIBUTES=ip=1.2.3.5
      - ENDPOINT_TYPE=BORING

  # The Tempo service stores traces send to it by Grafana Agent, and takes
  # queries from Grafana to visualise those traces.
  tempo:
    image: grafana/tempo:2.3.0
    ports:
      - "3200:3200"
      - "4317:4317"
      - "4318:4318"
      - "9411:9411"
      - "55680:55680"
      - "55681:55681"
      - "14250:14250"
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - "./tempo/tempo.yaml:/etc/tempo.yaml"

  # The Loki service stores logs sent to it, and takes queries from Grafana
  # to visualise those logs.
  loki:
    image: grafana/loki:2.9.2
    ports:
      - "3100:3100"

  #mimir:
  #  image: grafana/mimir:2.10.4
  #  command: ["-ingester.native-histograms-ingestion-enabled=true", "-config.file=/etc/mimir.yaml"]
  #  ports:
  #    - "9009:9009"
  #  volumes:
  #    - "./mimir/mimir.yaml:/etc/mimir.yaml"

  k6:
    image: grafana/k6
    volumes:
      - "./k6:/scripts"
    environment:
      #- K6_PROMETHEUS_RW_SERVER_URL=http://mimir:9009/api/v1/push
      - K6_PROMETHEUS_RW_SERVER_URL=http://victoriametrics:8428/api/v1/write
      - K6_DURATION=3600s
      - K6_VUS=1
      - K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM=true
    restart: always
    command: ["run", "-o", "experimental-prometheus-rw", "/scripts/mythical-loadtest.js"]

  pyroscope:
    image: grafana/pyroscope:1.2.0
    ports:
      - "4040:4040"
    command: ["server"]

  victoriametrics:
    container_name: victoriametrics
    image: victoriametrics/victoria-metrics:v1.95.1
    ports:
      - 8428:8428
      - 8089:8089
      - 8089:8089/udp
      - 2003:2003
      - 2003:2003/udp
      - 4242:4242
    volumes:
      - vmdata:/storage
    command:
      - "--storageDataPath=/storage"
      - "--graphiteListenAddr=:2003"
      - "--opentsdbListenAddr=:4242"
      - "--httpListenAddr=:8428"
      - "--influxListenAddr=:8089"
      - "--vmalert.proxyURL=http://vmalert:8880"
    restart: always
  
  alertmanager:
    container_name: alertmanager
    image: prom/alertmanager:v0.25.0
    volumes:
      - ./alertmanager/alertmanager.yml:/config/alertmanager.yml
    command:
      - "--config.file=/config/alertmanager.yml"
    ports:
      - 9093:9093
    restart: always

  vmalert:
    container_name: vmalert
    image: victoriametrics/vmalert:v1.93.4
    depends_on:
      - "victoriametrics"
      - "alertmanager"
    ports:
      - 8880:8880
    volumes:
      - ./sloth/mythical-beasts-SLO-rules.yml:/etc/alerts/alerts.yml
    command:
      - "--datasource.url=http://victoriametrics:8428/" # Read the metrics
      - "--remoteRead.url=http://victoriametrics:8428/" # Read the alerts saved
      - "--remoteWrite.url=http://victoriametrics:8428/" # Write the alerts state
      - "--notifier.url=http://alertmanager:9093/"
      - "--rule=/etc/alerts/*.yml"
      # display source of alerts in grafana
      - "--external.url=http://grafana:3000" #grafana outside container
    restart: always